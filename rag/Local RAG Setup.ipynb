{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a543af-698d-467b-9c8d-70221e98b9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/sndd595x235c_v7mf5tr7hch0000gn/T/ipykernel_8433/499225695.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from markdown import Markdown\n",
    "from pdfminer.high_level import extract_text\n",
    "from docx import Document\n",
    "from io import StringIO\n",
    "\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "from nltk import tokenize\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import llamafile_client as llamafile\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b0b4c7-9aa5-46dd-b2bc-e17e0bbeffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kjam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for tokenization\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b14899-2ccf-4db4-a61d-e2c5fa8d6419",
   "metadata": {},
   "source": [
    "### Design Note\n",
    "\n",
    "This builds the documents from scratch every time. This is because I don't have that many documents and it's overkill to store a vector database when I only use this once a week or less. This also means I can switch out models easily to change things, or edit documents without having to track changes.\n",
    "\n",
    "You will need to set an environment variable DOCUMENT_PATH which points to the folder you are interested in searching.\n",
    "\n",
    "In a shell where you start your Jupyter notebook, try something like, but obviously modifying the directory below:\n",
    "\n",
    "`export=/Users/etc/your_folder/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe4ec01-7129-41ac-8b8d-49d7b71600f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_STORE = []\n",
    "\n",
    "# from https://stackoverflow.com/questions/761824/python-how-to-convert-markdown-formatted-text-to-text\n",
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "\n",
    "# patching Markdown\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "\n",
    "def unmark(text):\n",
    "    return __md.convert(text)\n",
    "\n",
    "def traverse_dir(directory=os.getenv(\"DOCUMENT_PATH\")):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            text = read_file(os.path.join(root, filename))\n",
    "            if text:\n",
    "                DOCUMENT_STORE.append({\n",
    "                    'file': str(os.path.join(root, filename)),\n",
    "                    'text': text,\n",
    "                    'last_read': datetime.now().isoformat()\n",
    "                })\n",
    "\n",
    "def read_file(filename):\n",
    "    if '.doc' in filename:\n",
    "        doc = Document(filename)\n",
    "        return '\\n'.join(p.text for p in doc.paragraphs)\n",
    "    elif '.pdf' in filename:\n",
    "        try:\n",
    "            return extract_text(filename)\n",
    "        except:\n",
    "            pass\n",
    "    elif '.md' in filename:\n",
    "        with open(filename) as md_file:\n",
    "            return unmark(md_file.read())\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe420471-7ee2-459c-bc32-388a53ade564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(DOCUMENT_STORE) < 10:\n",
    "    traverse_dir()\n",
    "\n",
    "len(DOCUMENT_STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d909f-ab3c-478f-a798-8fba22e00687",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Now your document store is built, you should see how many documents were successfully parsed above. This is where we first do some basic NLP to search and retrieve using tried and true methods for search and retrieval, like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) because keywords produce better and more reliable search for finding relevant documents than AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a417ffe-769c-40d8-80aa-6aa06a49b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([d.get('text') for d in DOCUMENT_STORE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7df13a5-3400-4f44-a0ce-0c76394999d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', ..., '𝑚𝑖𝑛', '𝒟1', '𝜆ℒ'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0786f2-cc50-4d79-b099-1ee012fedd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [doc.get('text') for doc in DOCUMENT_STORE]\n",
    "tokenized_corpus_per_document = [tokenize.word_tokenize(doc.get('text')) for doc in DOCUMENT_STORE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3c0850-9872-41c9-9307-b1b3f3984371",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus_per_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a6caa-df48-4813-966e-04cdb0df0e96",
   "metadata": {},
   "source": [
    "Now you can change the query below to whatever it is that you want to search for!\n",
    "\n",
    "As you can see, I was doing some work to try to figure out what research articles to cite for my [memorization in AI systems articles](https://blog.kjamistan.com/a-deep-dive-into-memorization-in-deep-learning.html#a-deep-dive-into-memorization-in-deep-learning). :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a49ef7d-cd47-4ada-8581-3dd62db8c996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.33251386, 0.        , 2.83762931, 2.47679597, 0.        ,\n",
       "       3.31719706, 0.        , 3.28818993, 0.        , 2.98822339])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"memorization in AI systems\"\n",
    "tokenized_query = tokenize.word_tokenize(query)\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "doc_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f91ddb-0ffb-4872-9c91-32adcae7fdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article 0 Extracting Training Data from Diffusion Models  Nicholas Carlini∗1  Jamie Hayes∗2 Milad Nasr∗1  Matthew Jagielski+1  Vikash Sehwag+4  Florian Tram`er+3  Borja Balle†2 Daphne Ippolito†1  Eric Wallace†5  1Google 5UC Berkeley ∗Equal contribution +Equal contribution †Equal contribution  2DeepMind  4Princeton  3ETHZ  3 2 0 2  n a J  0 3  ]  R C . s c [  1 v 8\n",
      "----------------\n",
      "article 1 3 2 0 2  r a  M 6  ]  G L . s c [  3 v 6 4 6 7 0 . 2 0 2 2 : v i X r a  Published as a conference paper at ICLR 2023  QUANTIFYING MEMORIZATION ACROSS NEURAL LANGUAGE MODELS  Nicholas Carlini∗1  Katherine Lee1,3  Daphne Ippolito1,2 Florian Tramèr1\n",
      "----------------\n",
      "article 2 Search | arXiv e-print repository - Jagielski Created: June 18, 2024 4:30 PM URL: https://arxiv.org/search/cs?searchtype=author&query=Jagielski%2C+M Searching in archive cs. Search in all archives. Search term or terms All fieldsTitleAuthor(s)AbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDLicense (URI)arXiv author IDHelp pagesFull text Show abstracts    Hide abstracts Advanced Search 2550100200  results per page. Sort results by  Announcement date (newest first)Announcement date (oldest first)Submission date (newest first)Submission date (oldest first)Relevance   cs.LG cs.CR     arXiv:2406.08039  [pdf, other] Beyond the Mean: Differentially Private Prototypes for Private Transfer Learning Abstract:  Machine learning (ML) models have been shown to leak private information from their training datasets. Differential Privacy (DP), typically implemented through the differential private stochastic gradient descent algorithm (DP-SGD), has become the standard solution to bound leakage from the models. Despite recent improvements, DP-SGD-based approaches for private learning still usually struggle in… ▽ More Comments: Submitted to NeurIPS 2024 MSC Class: 68T01   cs.CR cs.CL cs.LG     arXiv:2405.20485  [pdf, other] Phantom: General Trigger Attacks on Retrieval Augmented Language Generation Authors: Harsh Chaudhari, Giorgio Severi, John Abascal, Matthew Jagielski, Christopher A. Choquette-Choo, Milad Nasr, Cristina Nita-Rotaru, Alina Oprea Abstract:  Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs) in chatbot applications, enabling developers to adapt and personalize the LLM output without expensive training or fine-tuning. RAG systems use an external knowledge database to retrieve the most relevant documents for a given query, providing this context to the LLM generator. While RAG achieves i… ▽ More Submitted 30 May, 2024; originally announced May 2024.   cs.LG      arXiv:2404.02052  [pdf, other] Noise Masking Attacks and Defenses for Pretrained Speech Models Authors: Matthew Jagielski, Om Thakkar, Lun Wang Abstract:  Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time,… ▽ More Submitted 2 April, 2024; originally announced April 2024. Comments: accepted to ICASSP 2024   cs.CR     arXiv:2403.06634  [pdf, other] Stealing Part of a Production Language Model Authors: Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick, Florian Tramèr Abstract:  We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\… ▽ More Submitted 11 March, 2024; originally announced March 2024.   cs.CR     arXiv:2402.09403  [pdf, other] Auditing Private Prediction Authors: Karan Chadha, Matthew Jagielski, Nicolas Papernot, Christopher Choquette-Choo, Milad Nasr Abstract:  Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of analgorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist forDP training algorithms. However machine learning can also be made private at inference. We propose thefirst framework for auditing private prediction where we instantiate adversaries with varying poiso… ▽ More\n",
      "----------------\n",
      "article 3 1 2 0 2  n u J  5 1  ]  R C . s c [  2 v 5 0 8 7 0 . 2 1 0 2 : v i X r a  Extracting Training Data from Large Language Models  Nicholas Carlini1 Ariel Herbert-Voss5,6 Dawn Song3  Florian Tramèr2 Katherine Lee1 Úlfar Erlingsson7 1Google 2Stanford 3UC Berkeley 4Northeastern University 5OpenAI\n",
      "----------------\n",
      "article 4 0 2 0 2  g u A 9  ]  G L . s c [  1 v 3 0 7 3 0 . 8 0 0 2 : v i X r a  What Neural Networks Memorize and Why: Discovering the Long Tail via Inﬂuence Estimation  Vitaly Feldman * † Apple  Chiyuan Zhang* Google Research, Brain Team  Abstract  Deep learning algorithms are well-known to have a propensity for ﬁtting the training data very well and often ﬁt\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for index, doc in enumerate(bm25.get_top_n(tokenized_query, doc_list, n=5)):\n",
    "    print(' '.join([\"article {}\".format(index)] + doc.split('\\n')[:50]))\n",
    "    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cd40b-90e6-4855-8429-6842d45eddda",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "The following is an alternative search to the above, which uses examples taken from [SBERT](https://www.sbert.net/index.html) documentation, specifically [this example notebook on searching Wikipedia entries from UKPLab](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb). You can mix and match as you like or as you see fit and modify the search function below to turn on things like cross-encoding. \n",
    "\n",
    "As you can see, they also use BM25 to first find the documents and then use the bi-encoder and cross-encoder to perform semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be907428-68c8-43c8-9730-6640aaf22343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aced3410-8acd-489e-a188-d37ed915a818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd760cd287a0425596a768c9c194e253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings = bi_encoder.encode([d.get('text') for d in DOCUMENT_STORE] , convert_to_tensor=True, show_progress_bar=True)\n",
    "#For now using full text, but in the example they use the first paragraph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eda49ed-efa9-46b8-8f47-397a15199b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, results=5, print_output=False, use_cross=False):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(tokenize.word_tokenize(query))\n",
    "    top_n = np.argpartition(bm25_scores, -results)[-results:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    if print_output:\n",
    "        print(\"Top-{} lexical search (BM25) hits\".format(results))\n",
    "        for hit in bm25_hits[0:3]:\n",
    "            print(\"\\t{:.3f}\\t{}\".format(hit['score'], DOCUMENT_STORE[hit['corpus_id']].get('text').replace(\"\\n\", \" \")[:150]))\n",
    "\n",
    "    ##### Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=results * 10) # before top_k was 32\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, DOCUMENT_STORE[hit['corpus_id']].get('text')] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of hits from bi-encoder\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    if print_output:\n",
    "        print(\"\\n-------------------------\\n\")\n",
    "        print(\"Top-{} Bi-Encoder Retrieval hits\".format(results))\n",
    "        for hit in hits[0:results]:\n",
    "            print(\"\\t{:.3f}\\t{}\".format(hit['score'], DOCUMENT_STORE[hit['corpus_id']].get('text').replace(\"\\n\", \" \")[:150]))\n",
    "\n",
    "    if use_cross:\n",
    "        hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "        # Output of hits from re-ranker\n",
    "        if print_output:\n",
    "            print(\"\\n-------------------------\\n\")\n",
    "            print(\"Top-{} Cross-Encoder Re-ranker hits\".format(results))\n",
    "        for hit in hits[0:results]:\n",
    "            print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], DOCUMENT_STORE[hit['corpus_id']].get('text').replace(\"\\n\", \" \")[:150]))\n",
    "\n",
    "    for h in hits:\n",
    "        h[\"file_name\"] = DOCUMENT_STORE[h[\"corpus_id\"]].get('file')\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dab9e8-c8be-47b6-bc6a-926358e72beb",
   "metadata": {},
   "source": [
    "To test it out and compare with the previous results, put your query below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f1834a-27c9-41e1-b8cb-988ce012ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: how does memorization work in deep learning?\n"
     ]
    }
   ],
   "source": [
    "hits = search(\"how does memorization work in deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5985c90-0d35-44cf-97b9-6dd1fde8c181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 752,\n",
       "  'score': 0.687652587890625,\n",
       "  'cross-score': np.float32(4.219793),\n",
       "  'file_name': '/Users/kjam/Documents/reading backlog/TODO-READonce-does-learning-require-memorization.pdf'},\n",
       " {'corpus_id': 94,\n",
       "  'score': 0.5995245575904846,\n",
       "  'cross-score': np.float32(-1.947927),\n",
       "  'file_name': '/Users/kjam/Documents/probably_private_notion_links/[2202 07646] Quantifying Memorization Across Neura d6f67ee82e0448358e9f2884239d7336.md'},\n",
       " {'corpus_id': 693,\n",
       "  'score': 0.5987856388092041,\n",
       "  'cross-score': np.float32(-1.9506565),\n",
       "  'file_name': '/Users/kjam/Documents/probably_private_notion_links/[2202 07646] Quantifying Memorization Across Neura d1d859725c9e4871847124ad89aa73c5.md'},\n",
       " {'corpus_id': 26,\n",
       "  'score': 0.5620447397232056,\n",
       "  'cross-score': np.float32(-2.0983696),\n",
       "  'file_name': '/Users/kjam/Documents/probably_private_notion_links/Do Machine Learning Models Memorize or Generalize 151062a01ace47d486adcb95dfd7f309.md'},\n",
       " {'corpus_id': 735,\n",
       "  'score': 0.560981810092926,\n",
       "  'cross-score': np.float32(-2.2681775),\n",
       "  'file_name': '/Users/kjam/Documents/reading backlog/READ-carlini-quantifying-memorization.pdf'},\n",
       " {'corpus_id': 729,\n",
       "  'score': 0.5603488683700562,\n",
       "  'cross-score': np.float32(3.2013135),\n",
       "  'file_name': '/Users/kjam/Documents/reading backlog/READ-nn-memorization.pdf'},\n",
       " {'corpus_id': 101,\n",
       "  'score': 0.5496671199798584,\n",
       "  'cross-score': np.float32(0.33425894),\n",
       "  'file_name': '/Users/kjam/Documents/probably_private_notion_links/[2112 12938] Counterfactual Memorization in Neural 16c80ad1f3db4167a6df8c48914e8141.md'},\n",
       " {'corpus_id': 466,\n",
       "  'score': 0.5322737693786621,\n",
       "  'cross-score': np.float32(0.15486063),\n",
       "  'file_name': '/Users/kjam/Documents/probably_private_notion_links/[2112 12938] Counterfactual Memorization in Neural 8622238cf04745c996e77c522fdb7373.md'},\n",
       " {'corpus_id': 623,\n",
       "  'score': 0.5312893986701965,\n",
       "  'cross-score': np.float32(4.005443),\n",
       "  'file_name': '/Users/kjam/Documents/probably_private_notion_links/interesting regularization research on stopping me 1826e4c7c21947b7a5db3b59c498ce3b.md'},\n",
       " {'corpus_id': 580,\n",
       "  'score': 0.5261462926864624,\n",
       "  'cross-score': np.float32(5.468516),\n",
       "  'file_name': '/Users/kjam/Documents/probably_private_notion_links/[2203 12171] An Empirical Study of Memorization in 884dfcf67134418e80211ae3f4e4e405.md'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e2fab-d448-4791-ba4e-8780362baf18",
   "metadata": {},
   "source": [
    "Now you are ready to use these results to interact with an AI chat assistant. You'll want to [install llamafiles for the models you want to use](https://github.com/Mozilla-Ocho/llamafile), and then you need to have one running in the background before you run the next few cells. You can do that by following the llamafile instructions on the README.\n",
    "\n",
    "The llamafile will get the prompt outlined in the prompt_template below, so feel free to modify as you see fit and experiment!\n",
    "\n",
    "Note: The helper files in this folder to start and use the llama client are taken and modified from [Mozilla's example RAG system](https://github.com/Mozilla-Ocho/llamafile-rag-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088bfcf-5612-4514-bf39-56c73d32bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llamafile(query, hit_list, num=5):\n",
    "    prompt_template = (\n",
    "        \"You are an expert Q&A system. Answer the user's query using the provided context information.\"\n",
    "        \"Summarize key points of the context information as part of your answer.\\n\"\n",
    "        \"Context information:\\n\"\n",
    "        \"%s\\n\"\n",
    "        \"Query: %s\"\n",
    "    )\n",
    "    indeces = set(h[\"corpus_id\"] for h in hit_list[:num-1])\n",
    "    text_bits = \" \".join(itertools.chain.from_iterable(DOCUMENT_STORE[i].get(\"text\").split('\\n')[:100] for i in indeces))\n",
    "    # this takes only the first 100 lines from the documents (sometimes a line/sometimes paragraph), so test different values\n",
    "    prompt = prompt_template % (text_bits, query)\n",
    "    prompt_ntokens = len(llamafile.tokenize(prompt, port=8080))\n",
    "    print(f\"(prompt_ntokens: {prompt_ntokens})\")\n",
    "    \n",
    "    print(\"=== Answer ===\")\n",
    "    answer = llamafile.completion(prompt)\n",
    "    print(f'\"{answer}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23966fd-64bb-48e9-8742-8077aefd96de",
   "metadata": {},
   "source": [
    "Depending on the length of your documents and the speediness of your computer, the next cell could take a minute. You can keep track of it by looking at the shell that is running your llamafile. \n",
    "\n",
    "You can also test different models to see if one meets your speed and accuracy requirements. Start by reading through [Mozilla's advice on which models work for RAGs](https://future.mozilla.org/builders/news_insights/llamafiles-for-embeddings-in-local-rag-applications/), but certainly test a few for your data and use case.\n",
    "\n",
    "In the meantime, I recommend refreshing your water, tea or coffee and doing a yoga pose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f834af87-7154-48a3-ba4c-95c320c92d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(prompt_ntokens: 4526)\n",
      "=== Answer ===\n",
      "\".\n",
      "\n",
      "Memorization in AI systems is a concern that has been raised due to the ability of large language models (LLMs) to memorize and repeat training data verbatim when prompted appropriately. This raises issues related to privacy, utility, and fairness. In a recent paper, the authors quantify memorization across different neural language models, finding that different models exhibit varying levels of memorization. They also propose methods to mitigate memorization, such as training with fewer examples or using adversarial training techniques. Overall, the paper highlights the importance of understanding and addressing memorization in AI systems to ensure privacy, utility, and fairness.</s>\"\n"
     ]
    }
   ],
   "source": [
    "ask_llamafile(query, hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d089668-3797-488e-ba31-dbe1ed766ed2",
   "metadata": {},
   "source": [
    "Putting it all together, you can use the following function and example below, or modify as you like based on your own experimentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14ed4892-8468-445a-ad03-e3aec8bd8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_respond(query, num=5):\n",
    "    hits = search(query, results=num)\n",
    "    print(\"related documents: {}\".format(', '.join(h.get('file_name') for h in hits[:num-1])))\n",
    "    ask_llamafile(query, hits, num=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9b50d91-7de0-4a25-aff8-64172fb45370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: what are the key factors linking memorization in AI systems to privacy?\n",
      "related documents: /Users/kjam/Documents/probably_private_notion_links/[2206 10469] The Privacy Onion Effect Memorization 8a3a8280682e413d918aa40de271ba3a.md, /Users/kjam/Documents/reading backlog/SSRN-id4713111.pdf, /Users/kjam/Documents/probably_private_notion_links/[1803 10266] Privacy-preserving Prediction b4eedef0ead74c3198ae2e6eeecf52fa.md, /Users/kjam/Documents/probably_private_notion_links/Leveraging transfer learning for large scale diffe efa775593e4f492badff4f9cdf3806c4.md\n",
      "(prompt_ntokens: 3343)\n",
      "=== Answer ===\n",
      "\"  \n",
      "Answer: Memorization in AI systems can lead to privacy issues because the model associates the data points it sees during training with the outcomes it generates. This can result in the memorization of sensitive information, making it vulnerable to privacy attacks. Additionally, removing certain outlier points from the data can expose the model to new privacy risks. The existence of the onion effect highlights the need for more effective privacy-enhancing technologies and the need to reevaluate the effectiveness of existing proposals in light of this phenomenon. It also suggests that privacy-enhancing technologies such as machine unlearning could harm the privacy of other users if not implemented properly.</s>\"\n"
     ]
    }
   ],
   "source": [
    "search_and_respond(\"what are the key factors linking memorization in AI systems to privacy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb6038-73e5-4fc9-87e7-ca0155116596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
